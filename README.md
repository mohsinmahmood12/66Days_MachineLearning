# Journey of 66Days_MachineLearning

![Images](https://github.com/mohsinmahmood12/66Days_MachineLearning/blob/main/Images/ML.jpeg)

**Day1 of 66DaysOfData!**
  
  **ðŸ’¡ Logistic Regression:**
  - Logistic Regression is the appropriate regression analysis to conduct when the dependent variable is binary. It is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level variables.
  - Binary or Binomial Logistic Regression can be understood as the type of Logistic Regression that deals with scenarios wherein the observed outcomes for dependent variables can be only in binary, i.e., it can have only two possible types.
  - Multinomial Logistic Regression works in scenarios where the outcome can have more than two possible types â€“ type A vs type B vs type C â€“ that are not in any particular order.
  
![Images](https://github.com/mohsinmahmood12/66Days_MachineLearning/blob/main/Images/01_LogisticRegression.png)


**Day2 of 66DaysOfData!**
  
  **ðŸ’¡ Gradient Descent:**
  - It is an algorithm to find the minimum of a convex function.  It is used in algorithm, for example, in linear regression.
  Gradient Descent is a popular optimization algorithm used in machine learning and deep learning for minimizing the cost or loss function of a model by iteratively adjusting the model parameters. The algorithm uses the gradient of the cost function to update the model parameters in the direction of steepest descent, i.e., towards the minimum value of the cost function.
    
        There are three types of Gradient Descent:
              i. Batch Gradient Descent
              ii. Stochastic Gradient Descent
              iii. Mini Batch Gradient Descent

         Steps to achieve minimal loss:
              1. Decide your cost function.
              2. Choose random initial values for parameters Î¸, 
              3. Find derivative of your cost function, 
              4. Choosing appropriate learning rate, 
              5. Update your parameters till you converge. This is where, you have found optimal Î¸ values where your cost function, is minimum.

![Images](https://github.com/mohsinmahmood12/66Days_MachineLearning/blob/main/Images/GradientDescent.png)

